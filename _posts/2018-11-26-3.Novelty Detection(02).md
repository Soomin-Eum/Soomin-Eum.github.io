---

layout: post

title: "Density-based Novelty Detection"

category: Business Analytics

comments: true

last_modified_at: 2018-11-26T14:25:52-05:00

---

안녕하세요. 고려대학교 LTIS 연구실 석사과정 음수민입니다.  

이번 포스팅에서는 Density-based Novelty Detection에 대해 살펴보도록 하겠습니다.  

본 글은 고려대 강필성 교수님의 강의자료를 바탕으로 정리되었습니다.  


  
# Novelty Detection  
  
우리는 새로운 관측치가 기존 관측치와 동일한 분포에 속하는지 혹은 다른 분포에 속하는지 여부를 결정할 수 있어야 합니다.
이러한 기능은 주로 두가지로 구분할 수 있습니다. 첫째, Outlier Detection 입니다. 이는 일반적으로 가장 집중되어있는 데이터와 멀리 떨어져 있는 관측치를 특이치로 정의하는 것입니다. 
둘째, Novelty Detection 입니다. 이는 새로운 관측지를 학습데이터로 사용하지 않으며 Outlier Detection이전에 제거될 수도 있습니다.    

 ![Novelty detection.png](https://scikit-learn.org/stable/_images/sphx_glr_plot_oneclass_0011.png)  
 
## Novel data란?

>“Observations that deviate so much from other observations as to arouse suspicions that they were generated by a different mechanism(Hawkins, 1980)” “Instances that their true probability density is very low(Harmelinget al., 2006)”

이상치에 대한 정확환 정의는 위와 같습니다. 쉽게 말해 다른 관측치보다 많이 벗어나 있는 관측치가 이상치인 것입니다. **여기서 주의해야할 점은 novel data와 noise data는 다르다**는 것입니다. Noise는 랜덤 에러로서 데이터 처리과정에서 제거해야하는 부분이라면 novel data는 우리가 찾고자 하는 관측치인 것입니다.   
 

# Density-based Novelty Detection 

밀도에 기반한 Novelty Detection(이하 이상치 탐지)의 목적은 데이터의 분포를 추정하고 훈련된 밀도 함수에 따라 새로운 관측치가 낮은 확률을 취한다면 특이값으로 정의를 내리는 것입니다. 

 ![Low density region](https://github.com/Soomin-Eum/Soomin-Eum.github.io/blob/master/images/1.PNG?raw=true)  


밀도기반 이상치 탐지법에는 일반적으로 가우시간 밀도 추정법(Gaussian density estimation), 혼합 가우시간 밀도 추정법(Mixture of Gassian estimation), 커널 밀도 추정법(Kernel density estimation),파젠 윈도우 밀도 추정법(Parzen window density estimation), 로컬 아웃라이어 팩터(Local Outlier factors)가 있습니다. 가장 먼저 가우시안 밀도 추정법에 대해 설명 드리겠습니다.  

## Gaussian Density Estimation

가우시안 밀도 추정법은 데이터가 하나의 정규분포를 따른다고 가정합니다. 

![Gaussian Density Estimation](https://github.com/Soomin-Eum/Soomin-Eum.github.io/blob/master/images/2.PNG?raw=true)  

![1](https://github.com/Soomin-Eum/Soomin-Eum.github.io/blob/master/images/3.PNG?raw=true)  

위 식에서 
$$ 
X+ 
$$는 정상영역을 뜻합니다.  
  
  

가우시간 밀도 추정법에는 먼저 데이터를 스케일링하는데 민감하지 않다는 장점이 있습니다. 데이터의 각 변수별로 분산을 고려하기 때문에 따로 스케일링 하지 않아도 분산까지 고려하게 됩니다. 또한 식 자체가 주어져 있기 때문에 추가적인 분석이 가능하다는 장점이 있습니다. 함수를 미분하는 등 최적의 임계값을 계산하는 것이 가능합니다.   


## Code Implementation
다음은 파이썬으로 작성된 코드 예시를 참조하겠습니다. 데이터는 아이리스 데이터로 사용되었습니다. 

<pre><code>
import pandas as pd
df = pd.read_csv("C:/Users/Sooomin Eum/Desktop/iris_virginica.csv")
#단별량 데이터 구현
y = pd.DataFrame.as_matrix(df[['sepallength']])
sns.distplot(y, bins=20, kde = False)
#package seabon을 이용하여 gaussian분포 추정
sns.distplot(y, fit=stats.norm, bins=20, kde=False,)
class Gaussian:
    def __init__(self, mu, sigma):
        self.mu = mu
        self.sigma = sigma

    def pdf(self, data): # 가우시안 분포 pdf 값 return
        u = (data - self.mu) / abs(self.sigma)
        y = (1 / (sqrt(2 * pi) * abs(self.sigma))) * exp(-u * u / 2)
        return y
best= Gaussian(np.mean(y), np.std(y))
print("best mu=" , best.mu)
print("best sigma=", best.sigma)
# 가우시안 분포 그리기
x = np.linspace(3,9,200)
g_single = stats.norm(best.mu, best.sigma).pdf(x)
sns.distplot(y, bins=20, kde = False, norm_hist= True)
plt.plot(x,g_single, label = 'Single Gaussian')
plt.legend()

print(y[0:5])

#정상 boundary 외 outlier filtering
n = 0
b=0
for i in range(0,y.shape[0]):
    if (stats.norm(best.mu, best.sigma).pdf(y[i])) >0.05 and (stats.norm(best.mu, best.sigma).pdf(y[i])) < 0.995:
        print(y[i],"= normal")
        n=n+1
    else:
        print(y[i],"=abnormal")
        b=b+1

print("normal=",n)
print("abnormal=",b)

</code></pre>
